#!/bin/bash
# Kubernetes Resource Deployment Script
# This script deploys all Kubernetes resources in the correct order
# Usage: ./deploy-resources.sh [namespace]

set -euo pipefail

# Configuration
NAMESPACE="${1:-api-template-prod}"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
K8S_BASE="${SCRIPT_DIR}/../base"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Helper functions
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_step() {
    echo -e "${BLUE}[STEP]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

check_prerequisites() {
    log_info "Checking prerequisites..."
    
    if ! command -v kubectl &> /dev/null; then
        log_error "kubectl is not installed"
        exit 1
    fi
    
    if ! kubectl cluster-info &> /dev/null; then
        log_error "Cannot connect to Kubernetes cluster"
        exit 1
    fi
    
    if [ ! -d "${K8S_BASE}" ]; then
        log_error "Kubernetes base directory not found: ${K8S_BASE}"
        exit 1
    fi
    
    log_info "Prerequisites check passed"
}

check_secrets() {
    log_info "Checking if required secrets exist..."
    
    local secrets=(
        "postgres-secrets"
        "postgres-tls"
        "postgres-ca"
        "redis-secrets"
        "app-secrets"
    )
    
    local missing_secrets=()
    for secret in "${secrets[@]}"; do
        if ! kubectl get secret "${secret}" -n "${NAMESPACE}" &> /dev/null; then
            missing_secrets+=("${secret}")
        fi
    done
    
    if [ ${#missing_secrets[@]} -gt 0 ]; then
        log_error "Missing required secrets: ${missing_secrets[*]}"
        log_error "Please run: ./create-secrets.sh ${NAMESPACE}"
        exit 1
    fi
    
    log_info "All required secrets are present"
}

deploy_namespace() {
    log_step "1/9 - Creating namespace..."
    kubectl apply -f "${K8S_BASE}/namespace/namespace.yaml"
    log_info "✓ Namespace created"
    echo ""
}

deploy_storage() {
    log_step "2/9 - Creating persistent volume claims..."
    kubectl apply -f "${K8S_BASE}/storage/persistentvolumeclaims.yaml"
    log_info "✓ Storage resources created"
    echo ""
}

sync_configs() {
    log_step "3/9 - Syncing configuration files..."
    
    # Check if deploy-config.sh exists
    if [ ! -f "${SCRIPT_DIR}/deploy-config.sh" ]; then
        log_error "deploy-config.sh not found"
        exit 1
    fi
    
    # Run deploy-config.sh in sync-only mode
    log_info "Copying source files to k8s/base/.k8s-sources/..."
    if "${SCRIPT_DIR}/deploy-config.sh" --sync-only > /dev/null 2>&1; then
        log_info "✓ Configuration files synced"
    else
        log_error "Failed to sync configuration files"
        exit 1
    fi
    echo ""
}

deploy_configmaps() {
    log_step "4/10 - Creating ConfigMaps via Kustomize..."
    
    # Kustomize will auto-generate ConfigMaps from .k8s-sources/
    log_info "ConfigMaps will be auto-generated by Kustomize in next step"
    log_info "✓ Ready to generate ConfigMaps"
    echo ""
}

deploy_with_kustomize() {
    log_step "5/10 - Deploying all resources with Kustomize..."
    
    log_info "Building and applying Kustomize resources..."
    
    # Try to apply with kubectl apply -k
    if kubectl apply -k "${K8S_BASE}" 2>&1 | tee /tmp/kustomize-deploy.log; then
        log_info "✓ All resources deployed (namespace, storage, ConfigMaps, services, deployments, jobs)"
    else
        # Check if the error is due to immutable field (selector)
        if grep -q "field is immutable" /tmp/kustomize-deploy.log; then
            log_warn "Detected immutable field errors (likely spec.selector or spec.template changes)"
            log_info "Extracting affected deployments and jobs..."
            
            # Extract deployment names from error messages
            local failed_deployments=$(grep "Deployment.apps.*is invalid" /tmp/kustomize-deploy.log | \
                sed -E 's/.*Deployment\.apps "([^"]+)".*/\1/' | sort -u)
            
            # Extract job names from error messages
            local failed_jobs=$(grep "Job.batch.*is invalid" /tmp/kustomize-deploy.log | \
                sed -E 's/.*Job\.batch "([^"]+)".*/\1/' | sort -u)
            
            if [ -n "$failed_deployments" ] || [ -n "$failed_jobs" ]; then
                if [ -n "$failed_deployments" ]; then
                    log_info "Deployments to recreate: $(echo $failed_deployments | tr '\n' ' ')"
                    for deployment in $failed_deployments; do
                        log_info "Deleting deployment: $deployment"
                        kubectl delete deployment "$deployment" -n "${NAMESPACE}" --ignore-not-found=true
                    done
                fi
                
                if [ -n "$failed_jobs" ]; then
                    log_info "Jobs to recreate: $(echo $failed_jobs | tr '\n' ' ')"
                    for job in $failed_jobs; do
                        log_info "Deleting job: $job"
                        kubectl delete job "$job" -n "${NAMESPACE}" --ignore-not-found=true
                    done
                fi
                
                log_info "Waiting for resources to be deleted..."
                sleep 5
                
                log_info "Retrying Kustomize deployment..."
                if kubectl apply -k "${K8S_BASE}"; then
                    log_info "✓ All resources deployed after recreating resources"
                else
                    log_error "Kustomize deployment failed even after recreating resources"
                    exit 1
                fi
            else
                log_error "Could not extract deployment/job names from error messages"
                log_error "Please check /tmp/kustomize-deploy.log for details"
                exit 1
            fi
        else
            log_error "Kustomize deployment failed with non-immutable field errors"
            log_error "Check /tmp/kustomize-deploy.log for details"
            exit 1
        fi
    fi
    
    # Clean up temp log
    rm -f /tmp/kustomize-deploy.log
    echo ""
}

wait_for_databases() {
    log_step "6/10 - Waiting for databases and caches..."
    
    log_info "Waiting for PostgreSQL to be ready..."
    if kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=postgres -n "${NAMESPACE}" --timeout=120s; then
        log_info "✓ PostgreSQL is ready"
    else
        log_error "PostgreSQL failed to become ready"
        log_info "Checking PostgreSQL logs..."
        kubectl logs -n "${NAMESPACE}" -l app.kubernetes.io/name=postgres --tail=50 || true
        exit 1
    fi
    
    log_info "Waiting for Redis to be ready..."
    if kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=redis -n "${NAMESPACE}" --timeout=120s; then
        log_info "✓ Redis is ready"
    else
        log_error "Redis failed to become ready"
        log_info "Checking Redis logs..."
        kubectl logs -n "${NAMESPACE}" -l app.kubernetes.io/name=redis --tail=50 || true
        exit 1
    fi
    echo ""
}

wait_for_temporal_setup() {
    log_step "7/10 - Waiting for Temporal schema initialization..."
    
    log_info "Waiting for Temporal schema setup to complete (this may take a few minutes)..."
    if kubectl wait --for=condition=complete job/temporal-schema-setup -n "${NAMESPACE}" --timeout=300s 2>/dev/null; then
        log_info "✓ Temporal schemas initialized"
    else
        log_warn "Temporal schema setup job may have failed or timed out"
        log_info "Checking job logs..."
        kubectl logs -n "${NAMESPACE}" job/temporal-schema-setup --tail=50 || true
        log_warn "Continuing anyway - Temporal may initialize schemas automatically"
    fi
    echo ""
}

wait_for_temporal() {
    log_step "8/10 - Waiting for Temporal server..."
    
    log_info "Waiting for Temporal to be ready..."
    if kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=temporal -n "${NAMESPACE}" --timeout=120s; then
        log_info "✓ Temporal is ready"
    else
        log_error "Temporal failed to become ready"
        log_info "Checking Temporal logs..."
        kubectl logs -n "${NAMESPACE}" -l app.kubernetes.io/name=temporal --tail=50 || true
        exit 1
    fi
    echo ""
}

wait_for_app() {
    log_step "9/10 - Waiting for application..."
    
    log_info "Waiting for application to be ready..."
    if kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=app -n "${NAMESPACE}" --timeout=120s; then
        log_info "✓ Application is ready"
    else
        log_error "Application failed to become ready"
        log_info "Checking application logs..."
        kubectl logs -n "${NAMESPACE}" -l app.kubernetes.io/name=app --tail=50 || true
        exit 1
    fi
    echo ""
}

verify_deployment() {
    log_step "10/10 - Verifying deployment..."
    
    echo ""
    log_info "Pod Status:"
    kubectl get pods -n "${NAMESPACE}" -o wide
    
    echo ""
    log_info "Service Status:"
    kubectl get svc -n "${NAMESPACE}"
    
    echo ""
    log_info "Checking application health..."
    
    # Get app pod name
    local app_pod=$(kubectl get pods -n "${NAMESPACE}" -l app.kubernetes.io/name=app -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
    
    if [ -n "${app_pod}" ]; then
        log_info "Checking app logs for health status..."
        if kubectl logs -n "${NAMESPACE}" "${app_pod}" | grep -q "Application startup complete"; then
            log_info "✓ Application started successfully"
            
            # Check for healthy services
            if kubectl logs -n "${NAMESPACE}" "${app_pod}" | grep -q "✓ Database is healthy"; then
                log_info "  ✓ Database is healthy"
            fi
            if kubectl logs -n "${NAMESPACE}" "${app_pod}" | grep -q "✓ Redis is healthy"; then
                log_info "  ✓ Redis is healthy"
            fi
            if kubectl logs -n "${NAMESPACE}" "${app_pod}" | grep -q "✓ Temporal is healthy"; then
                log_info "  ✓ Temporal is healthy"
            fi
        else
            log_warn "Application may not have started properly. Recent logs:"
            kubectl logs -n "${NAMESPACE}" "${app_pod}" --tail=20
        fi
    fi
    echo ""
}

print_next_steps() {
    log_info "=== Deployment Complete ==="
    echo ""
    log_info "Next steps:"
    echo "  1. Access the application:"
    echo "     kubectl port-forward -n ${NAMESPACE} svc/app 8000:8000"
    echo "     curl http://localhost:8000/health"
    echo ""
    echo "  2. Access Temporal Web UI:"
    echo "     kubectl port-forward -n ${NAMESPACE} svc/temporal-web 8080:8080"
    echo "     open http://localhost:8080"
    echo ""
    echo "  3. View logs:"
    echo "     kubectl logs -n ${NAMESPACE} -l app.kubernetes.io/name=app -f"
    echo ""
    echo "  4. Check pod status:"
    echo "     kubectl get pods -n ${NAMESPACE}"
    echo ""
}

main() {
    log_info "Starting Kubernetes resource deployment to namespace: ${NAMESPACE}"
    log_info "Kubernetes base directory: ${K8S_BASE}"
    log_info "Using Kustomize for unified ConfigMap generation"
    echo ""
    
    check_prerequisites
    check_secrets
    
    echo ""
    sync_configs
    deploy_configmaps
    deploy_with_kustomize
    wait_for_databases
    wait_for_temporal_setup
    wait_for_temporal
    wait_for_app
    verify_deployment
    
    echo ""
    log_info "✓✓✓ Deployment completed successfully! ✓✓✓"
    echo ""
    print_next_steps
}

# Run main function
main
