# Kubernetes Deployment Guide

## Quick Start - Automated Deployment

For a completely automated deployment, run these four scripts in order:

```bash
# 1. Build all Docker images
./k8s/scripts/build-images.sh

# 2. Generate secrets (TLS certs, passwords, etc.)
cd infra/secrets && ./generate_secrets.sh && cd ../..

# 3. Create Kubernetes secrets from generated files
./k8s/scripts/create-secrets.sh

# 4. Deploy all resources in correct order
./k8s/scripts/deploy-resources.sh
```

That's it! The scripts will handle everything including waiting for services to be ready.

---

## Detailed Step-by-Step Guide

### 1. Build Docker Images

```bash
# Make the build script executable
chmod +x k8s/scripts/build-images.sh

# Build all images for local cluster (Minikube/kind/k3d)
./k8s/scripts/build-images.sh
```

### 2. Generate Secrets

```bash
# Generate all required secrets
cd infra/secrets
./generate_secrets.sh

# Secrets will be created in infra/secrets/ directory
```

### 3. Create Kubernetes Secrets

Use the automated script:

```bash
# Create all Kubernetes secrets from generated files
./k8s/scripts/create-secrets.sh
```

The script will:
- Create the namespace if it doesn't exist
- Delete existing secrets (if any)
- Create all required secrets from `infra/secrets/` directory
- Verify all secrets were created successfully

**Optional**: Specify a different namespace:
```bash
./k8s/scripts/create-secrets.sh my-namespace
```

### 4. Deploy Resources

#### Option A: Automated Deployment (Recommended)

Use the automated deployment script:

```bash
# Deploy all resources in the correct order
./k8s/scripts/deploy-resources.sh
```

The script will:
1. Create namespace and storage resources
2. Deploy ConfigMaps
3. Create Services
4. Deploy PostgreSQL and Redis (and wait for them to be ready)
5. Initialize Temporal schemas
6. Deploy Temporal (and wait for it to be ready)
7. Deploy the application (and wait for it to be ready)
8. Verify all services are healthy
9. Display next steps for accessing the application

**Optional**: Specify a different namespace:
```bash
./k8s/scripts/deploy-resources.sh my-namespace
```

#### Option B: Using Kustomize

Now that `commonLabels` is disabled in `kustomization.yaml`, you can use kustomize:

```bash
kubectl apply -k k8s/base/
```

**Note**: This will deploy all resources at once without waiting for dependencies. Some pods may restart a few times until dependencies are ready (e.g., Temporal will restart until schemas are initialized).

#### Option C: Manual Deployment

If you prefer to deploy resources manually in the correct order:

```bash
# Create namespace
kubectl apply -f k8s/base/namespace/namespace.yaml

# Create storage
kubectl apply -f k8s/base/storage/persistentvolumeclaims.yaml

# Note: ConfigMaps are auto-generated by Kustomize from .k8s-sources/
# If deploying manually, first run: ./k8s/scripts/deploy-config.sh
# Then apply with kustomize to generate ConfigMaps

# Create Services
kubectl apply -f k8s/base/services/services.yaml

# Deploy databases and caches
kubectl apply -f k8s/base/deployments/postgres.yaml
kubectl apply -f k8s/base/deployments/redis.yaml

# Wait for databases to be ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=postgres -n api-template-prod --timeout=120s
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=redis -n api-template-prod --timeout=120s

# Initialize Temporal schemas (required before Temporal deployment)
kubectl apply -f k8s/base/jobs/temporal-schema-setup.yaml

# Wait for schema setup to complete
kubectl wait --for=condition=complete job/temporal-schema-setup -n api-template-prod --timeout=300s

# Deploy Temporal
kubectl apply -f k8s/base/deployments/temporal.yaml
kubectl apply -f k8s/base/deployments/temporal-web.yaml

# Wait for Temporal to be ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=temporal -n api-template-prod --timeout=120s

# Deploy application
kubectl apply -f k8s/base/deployments/app.yaml

# Wait for app to be ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=app -n api-template-prod --timeout=120s

# Deploy Temporal worker
kubectl apply -f k8s/base/deployments/worker.yaml

# Wait for worker to be ready
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=worker -n api-template-prod --timeout=120s
```

### 5. Verify Deployment

```bash
# Check all pods are running
kubectl get pods -n api-template-prod

# Expected output:
# NAME                                   READY   STATUS      RESTARTS   AGE
# app-xxxxx                              1/1     Running     0          1m
# worker-xxxxx                           1/1     Running     0          1m
# postgres-xxxxx                         1/1     Running     0          5m
# redis-xxxxx                            1/1     Running     0          5m
# temporal-xxxxx                         1/1     Running     0          3m
# temporal-web-xxxxx                     1/1     Running     0          3m
# temporal-admin-tools-xxxxx             1/1     Running     0          3m
# postgres-verifier-xxxxx                0/1     Completed   0          4m
# temporal-schema-setup-xxxxx            0/1     Completed   0          4m
# temporal-namespace-init-xxxxx          0/1     Completed   0          2m

# Check app logs
kubectl logs -n api-template-prod -l app.kubernetes.io/name=app --tail=50

# Look for:
# ✓ Database is healthy
# ✓ Redis is healthy
# ✓ Temporal is healthy
# Application startup complete.
```

### 6. Access Application

```bash
# Port forward to access the app locally
kubectl port-forward -n api-template-prod svc/app 8000:8000

# Test the health endpoint
curl http://localhost:8000/health
```

---

## Available Automation Scripts

The `k8s/scripts/` directory contains several automation scripts:

### `build-images.sh`
Builds all Docker images with correct build contexts and loads them into your local Kubernetes cluster.

**Usage**:
```bash
./k8s/scripts/build-images.sh
```

**Features**:
- Auto-detects cluster type (Minikube, kind, k3d, or Docker Desktop)
- Uses correct build contexts for each Dockerfile
- Validates all images were built successfully
- Loads images into the cluster automatically

### `create-secrets.sh`
Creates all Kubernetes secrets from the `infra/secrets/` directory.

**Usage**:
```bash
./k8s/scripts/create-secrets.sh [namespace]
```

**Features**:
- Creates namespace if it doesn't exist
- Deletes existing secrets to avoid conflicts
- Creates all required secrets:
  - PostgreSQL passwords and TLS certificates
  - Redis password
  - Application session/CSRF secrets
- Verifies all secrets were created successfully

### `deploy-resources.sh`
Deploys all Kubernetes resources in the correct order with automatic health checks.

**Usage**:
```bash
./k8s/scripts/deploy-resources.sh [namespace]
```

**Features**:
- Checks prerequisites (kubectl, cluster connectivity, secrets)
- Deploys resources in correct dependency order
- Waits for each service to be ready before proceeding
- Initializes Temporal schemas before deploying Temporal
- Verifies final deployment health
- Displays next steps for accessing services

**Deployment Order**:
1. Namespace
2. Storage (PVCs)
3. ConfigMaps
4. Services
5. Databases (PostgreSQL, Redis) - waits for ready
6. Temporal schema initialization - waits for completion
7. Temporal deployment - waits for ready
8. Application deployment - waits for ready
9. Verification and health checks

---

## About Kustomize

The `k8s/base/kustomization.yaml` file exists and can be used with `kubectl apply -k k8s/base/`.

### What Was Fixed

Previously, the kustomization.yaml had `commonLabels` enabled:
```yaml
commonLabels:
  app.kubernetes.io/managed-by: kustomize
  environment: production
```

**The Problem**: Kustomize's `commonLabels` feature adds labels to BOTH:
1. Resource metadata (✅ good)
2. Service selectors (❌ BAD)

When commonLabels were added to Service selectors, they didn't match the pod labels, causing connection failures:
```
redis.exceptions.ConnectionError: Error 111 connecting to redis:6379. Connection refused.
```

**The Fix**: We commented out `commonLabels` in kustomization.yaml. Now `kubectl apply -k` works correctly.

### Using Kustomize Safely

If you want to add common labels/annotations in the future:

**✅ Safe - Use commonAnnotations**:
```yaml
commonAnnotations:
  app.kubernetes.io/managed-by: kustomize
  deployment-date: "2025-11-09"
```
Annotations don't affect Service selectors.

**❌ Avoid - commonLabels with Services**:
Only use `commonLabels` if all your pod templates also have those labels, or use kustomize patches to exclude Services from getting the labels in their selectors.

---

## Configuration Management

### Single Source of Truth Pattern

The deployment uses a two-ConfigMap approach:

1. **`app-env.yaml`** - Environment variables (generated from `.env.example`)
   - Database connection settings
   - Redis configuration
   - OIDC placeholders
   - Session/CSRF file paths

2. **`app-config.yaml`** - Application config (generated from `config.yaml`)
   - Full application configuration
   - Contains `${VAR_NAME:-default}` patterns
   - Application performs runtime environment variable substitution

### Regenerating ConfigMaps

If you modify `config.yaml` or need to update environment variables:

```bash
# Regenerate app-config.yaml from config.yaml
kubectl create configmap app-config \
  --from-file=config.yaml=config.yaml \
  --dry-run=client -o yaml -n api-template-prod > k8s/base/configmaps/app-config.yaml.new

# Add metadata
cat > k8s/base/configmaps/app-config.yaml << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: api-template-prod
EOF
cat k8s/base/configmaps/app-config.yaml.new | grep -A 9999 "^data:" >> k8s/base/configmaps/app-config.yaml
rm k8s/base/configmaps/app-config.yaml.new

# Apply the updated ConfigMap
kubectl apply -f k8s/base/configmaps/app-config.yaml

# Restart app to pick up changes
kubectl rollout restart deployment/app -n api-template-prod
```

---

## Troubleshooting

### Pod Won't Start - Permission Denied

**Symptom**: Pods fail with permission errors when trying to modify config files or change ownership

**Solution**: Pods that use entrypoint scripts (postgres, redis, app) need specific Linux capabilities:

```yaml
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    add:
      - CHOWN
      - SETGID
      - SETUID
    drop:
      - ALL
```

Pods start as root, run the entrypoint script (which sets up files/permissions), then drop to non-root user via `gosu`/`su-exec`.

### Service Can't Reach Pods

**Symptom**: `Connection refused` errors when pods try to connect to services

**Check**: Ensure Service selectors match pod labels:

```bash
# Check service selector
kubectl get svc <service-name> -n api-template-prod -o jsonpath='{.spec.selector}'

# Check pod labels
kubectl get pods -n api-template-prod -l app.kubernetes.io/name=<service-name> --show-labels
```

### Temporal Won't Start - Schema Not Found

**Symptom**: `relation "schema_version" does not exist`

**Solution**: Run the temporal-schema-setup job BEFORE deploying Temporal:

```bash
kubectl apply -f k8s/base/jobs/temporal-schema-setup.yaml
kubectl wait --for=condition=complete job/temporal-schema-setup -n api-template-prod --timeout=300s
kubectl apply -f k8s/base/deployments/temporal.yaml
```

### App Crashes - Missing Environment Variables

**Symptom**: `ValueError: Required environment variable OIDC_GOOGLE_CLIENT_SECRET not set`

**Solution**: Ensure `app-env.yaml` ConfigMap is applied and contains all required variables. The app does NOT support disabling Temporal or OIDC providers - they must have placeholder values at minimum.

---

## Cleanup

```bash
# Delete all resources
kubectl delete namespace api-template-prod

# Remove local images (Minikube)
minikube ssh "docker images | grep api-template | awk '{print \$3}' | xargs docker rmi -f"
```

---

## Production Considerations

1. **Secrets**: Use a proper secret management solution (Vault, sealed-secrets, external-secrets)
2. **Images**: Push to a container registry, update image references
3. **Ingress**: Add an Ingress controller for external access
4. **TLS**: Enable TLS for all services (postgres, redis, temporal)
5. **Monitoring**: Add Prometheus, Grafana, logging aggregation
6. **Backups**: Configure automated backups for PostgreSQL and Redis
7. **Resource Limits**: Tune CPU/memory requests and limits based on load testing
8. **High Availability**: Increase replicas for all services
